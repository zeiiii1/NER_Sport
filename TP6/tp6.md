# TP6 â€“ Ã‰valuation du modÃ¨le NER fine-tunÃ©

---

## ğŸ¯ Objectif

Ã‰valuer les performances du modÃ¨le CamemBERT fine-tunÃ© Ã  la tÃ¢che de **Reconnaissance d'EntitÃ©s NommÃ©es (NER)** sur les articles sportifs.

---

## ğŸ“ MÃ©triques utilisÃ©es

- **PrÃ©cision (precision)** : parmi les entitÃ©s dÃ©tectÃ©es, combien sont correctes ?
- **Rappel (recall)** : parmi toutes les vraies entitÃ©s, combien ont Ã©tÃ© trouvÃ©es ?
- **F1-score** : moyenne harmonique prÃ©cision/rappel (Ã©quilibre entre les deux)

---

## ğŸ§° Outils

- [`transformers`](https://huggingface.co/docs/transformers/)
- [`seqeval`](https://github.com/chakki-works/seqeval) : mÃ©trique NER standard
- PyTorch

---

## ğŸ“ Organisation

TP6/
â”œâ”€â”€ tp6.md
â””â”€â”€ scripts/
â””â”€â”€ evaluate_model.py


---

## ğŸ“Œ ProcÃ©dure dâ€™Ã©valuation

1. Charger le modÃ¨le fine-tunÃ© depuis `TP5/model/`
2. Charger le jeu de test depuis `TP5/data/test.json`
3. Appliquer le modÃ¨le pour prÃ©dire les entitÃ©s sur chaque sÃ©quence
4. Comparer les Ã©tiquettes prÃ©dites avec les Ã©tiquettes rÃ©elles
5. Calculer les scores `precision`, `recall`, `f1-score` par classe et au global

---

## ğŸ“Š RÃ©sultat attendu

Un rapport comme :

          precision    recall  f1-score   support

   B-PER       0.91      0.88      0.90        40
   I-PER       0.87      0.85      0.86        30
   B-ORG       0.83      0.79      0.81        25
   I-ORG       0.78      0.75      0.76        20
       O       0.97      0.98      0.98       500
micro avg      0.95      0.95      0.95       615
macro avg      0.87      0.85      0.86       615
weighted avg   0.94      0.95      0.94       615

---

## ğŸ”œ AmÃ©liorations possibles

- Ajouter plus de donnÃ©es d'entraÃ®nement
- Raffiner lâ€™annotation des entitÃ©s
- Ajuster les hyperparamÃ¨tres (batch size, learning rate, etc.)

