# TP4 â€“ Augmentation de donnÃ©es & Choix du modÃ¨le

---

## ğŸ¯ Objectif

AmÃ©liorer la qualitÃ© et la diversitÃ© du corpus NER en crÃ©ant un **dataset synthÃ©tique**. Ensuite, identifier **lâ€™architecture de modÃ¨le** et un **modÃ¨le prÃ©-entraÃ®nÃ©** adaptÃ© Ã  notre tÃ¢che.

---

## ğŸ§ª 1. Augmentation de donnÃ©es (data augmentation)

### Pourquoi ?

- Le corpus de TP2 est limitÃ©.
- Les modÃ¨les de NER sâ€™amÃ©liorent avec des donnÃ©es variÃ©es.
- Augmenter les phrases permet dâ€™amÃ©liorer la robustesse et la gÃ©nÃ©ralisation du modÃ¨le.

### MÃ©thodes utilisÃ©es

1. **Paraphrases automatiques** (modification de la structure syntaxique)
2. **Remplacement d'entitÃ©s par d'autres du mÃªme type** (ex : remplacer "MbappÃ©" par "Griezmann")
3. **Insertion de nouvelles phrases artificielles** gÃ©nÃ©rÃ©es avec un modÃ¨le de langue

### Outils proposÃ©s

- [`nlpaug`](https://github.com/makcedward/nlpaug) : bibliothÃ¨que Python pour augmenter du texte.
- [`textattack`](https://github.com/TextAttack/TextAttack) : outils pour gÃ©nÃ©ration de paraphrases.
- ModÃ¨le LLM comme GPT-2, GPT-3 (via API) ou fine-tunÃ© localement pour produire du texte.

---

## ğŸ“ Exemple de donnÃ©e augmentÃ©e

**Original :**

Kylian MbappÃ© a marquÃ© un doublÃ© contre lâ€™Olympique de Marseille.

**Version synthÃ©tique :**

Griezmann a inscrit deux buts face Ã  lâ€™OM.

---

## ğŸ§  2. Choix de lâ€™architecture adaptÃ©e

La tÃ¢che de NER nÃ©cessite de traiter une sÃ©quence mot par mot et dâ€™assigner Ã  chaque token une Ã©tiquette.

### Architecture choisie : **Transformers avec Ã©tiquetage par token**

- ModÃ¨le de type `BERT` avec une tÃªte de classification token-level.
- Lâ€™entrÃ©e est une sÃ©quence de tokens â†’ chaque token reÃ§oit une prÃ©diction dâ€™Ã©tiquette (PER, LOC, ORG...).

---

## âœ… 3. ModÃ¨le choisi

### ğŸ§© ModÃ¨le : `CamemBERT` (de la famille BERT)

- PrÃ©entraÃ®nÃ© sur un large corpus franÃ§ais.
- Performant pour les tÃ¢ches de NER, POS, classification.
- Disponible via Hugging Face : `camembert-base`.

### IntÃ©gration avec `transformers` de Hugging Face

```python
from transformers import CamembertTokenizerFast, CamembertForTokenClassification

tokenizer = CamembertTokenizerFast.from_pretrained("camembert-base")
model = CamembertForTokenClassification.from_pretrained("camembert-base", num_labels=9)
```

ğŸ“ PrÃ©paration des donnÃ©es

Format CoNLL (token par ligne, sÃ©parÃ© par une tabulation)

Exemple :

Kylian  B-PER
MbappÃ©  I-PER
a       O
marquÃ©  O
...
